{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üó∫Ô∏è MIT-Level Interactive Research Dashboard\n",
    "## Multi-Agent Tour Guide System - Jupyter Integration\n",
    "\n",
    "**Research Level:** MIT / Academic Publication  \n",
    "**Version:** 1.0.0  \n",
    "**Date:** November 2025\n",
    "\n",
    "---\n",
    "\n",
    "This notebook provides:\n",
    "1. **Interactive Parameter Exploration** - Sliders and controls for real-time analysis\n",
    "2. **Publication-Quality Visualizations** - Plotly-based interactive charts\n",
    "3. **Statistical Analysis Tools** - Hypothesis testing and effect sizes\n",
    "4. **Monte Carlo Simulations** - Stochastic system modeling\n",
    "5. **Pareto Frontier Analysis** - Multi-objective optimization\n",
    "\n",
    "### üöÄ Quick Start\n",
    "\n",
    "For the full interactive dashboard experience, run:\n",
    "```bash\n",
    "python run_dashboard.py\n",
    "```\n",
    "\n",
    "This notebook provides a complementary Jupyter-based exploration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"..\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Plotly for interactive visualizations\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "\n",
    "# Set dark theme for Plotly\n",
    "pio.templates.default = \"plotly_dark\"\n",
    "\n",
    "# Dashboard components\n",
    "from src.dashboard.data_manager import DashboardDataManager, QueueConfig\n",
    "from src.dashboard.components import (\n",
    "    SensitivityPanel,\n",
    "    ParetoFrontierPanel,\n",
    "    StatisticalComparisonPanel,\n",
    "    MonteCarloPanel,\n",
    "    AgentPerformancePanel,\n",
    "    SystemMonitorPanel,\n",
    "    COLORS,\n",
    ")\n",
    "\n",
    "# Initialize data manager\n",
    "data_manager = DashboardDataManager()\n",
    "\n",
    "print(\"‚úÖ Dashboard components loaded successfully!\")\n",
    "print(\"\\nüìä Available panels:\")\n",
    "print(\"   ‚Ä¢ SensitivityPanel\")\n",
    "print(\"   ‚Ä¢ ParetoFrontierPanel\")\n",
    "print(\"   ‚Ä¢ StatisticalComparisonPanel\")\n",
    "print(\"   ‚Ä¢ MonteCarloPanel\")\n",
    "print(\"   ‚Ä¢ AgentPerformancePanel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. üé≤ Monte Carlo Simulation\n",
    "\n",
    "Run a comprehensive Monte Carlo simulation with the default configuration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "N_SIMULATIONS = 10000\n",
    "SOFT_TIMEOUT = 15.0  # seconds\n",
    "HARD_TIMEOUT = 30.0  # seconds\n",
    "\n",
    "print(f\"üé≤ Running Monte Carlo simulation with N={N_SIMULATIONS:,}...\")\n",
    "print(f\"   Configuration: soft_timeout={SOFT_TIMEOUT}s, hard_timeout={HARD_TIMEOUT}s\")\n",
    "\n",
    "# Run simulation\n",
    "config = QueueConfig(soft_timeout=SOFT_TIMEOUT, hard_timeout=HARD_TIMEOUT)\n",
    "baseline_df = data_manager.get_baseline_simulation(\n",
    "    N_SIMULATIONS, config, force_refresh=True\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Simulation complete!\")\n",
    "print(f\"\\nüìä Summary Statistics:\")\n",
    "print(f\"   Mean Latency:   {baseline_df['latency'].mean():.3f}s\")\n",
    "print(f\"   Std Latency:    {baseline_df['latency'].std():.3f}s\")\n",
    "print(f\"   Mean Quality:   {baseline_df['quality'].mean():.3f}\")\n",
    "print(f\"   Complete Rate:  {(baseline_df['status'] == 'complete').mean():.1%}\")\n",
    "print(f\"   Success Rate:   {(baseline_df['status'] != 'failed').mean():.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive simulation results visualization\n",
    "fig = MonteCarloPanel.create_simulation_results(baseline_df)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. üî¨ Interactive Sensitivity Analysis\n",
    "\n",
    "Explore how different parameters affect system performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run sensitivity analysis for soft_timeout\n",
    "print(\"üî¨ Running sensitivity analysis for soft_timeout...\")\n",
    "\n",
    "soft_timeout_values = np.linspace(5, 30, 11).tolist()\n",
    "sensitivity_df = data_manager.run_sensitivity_analysis(\n",
    "    \"soft_timeout\", soft_timeout_values, n_sims=3000\n",
    ")\n",
    "\n",
    "# Create interactive sensitivity visualization\n",
    "fig = SensitivityPanel.create_parameter_impact_chart(sensitivity_df, \"soft_timeout\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. üéØ Pareto Frontier Exploration\n",
    "\n",
    "Explore the quality-latency tradeoff space to find optimal configurations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Pareto frontier data\n",
    "print(\"üéØ Generating Pareto frontier analysis...\")\n",
    "print(\"   This may take a minute...\")\n",
    "\n",
    "pareto_df = data_manager.run_pareto_analysis(\n",
    "    soft_timeout_range=(5, 30), hard_timeout_range=(10, 60), n_points=49, n_sims=500\n",
    ")\n",
    "\n",
    "# 3D Surface and 2D Pareto visualization\n",
    "fig = ParetoFrontierPanel.create_3d_pareto_surface(pareto_df)\n",
    "fig.show()\n",
    "\n",
    "fig2 = ParetoFrontierPanel.create_pareto_scatter(pareto_df)\n",
    "fig2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. üìê Statistical A/B Comparison\n",
    "\n",
    "Compare two configurations with rigorous statistical testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define configurations to compare\n",
    "config_a = QueueConfig(soft_timeout=15.0, hard_timeout=30.0)  # Default\n",
    "config_b = QueueConfig(soft_timeout=8.0, hard_timeout=15.0)  # Aggressive\n",
    "\n",
    "print(\"üìê Comparing configurations:\")\n",
    "print(\n",
    "    f\"   Config A: soft={config_a.soft_timeout}s, hard={config_a.hard_timeout}s (Default)\"\n",
    ")\n",
    "print(\n",
    "    f\"   Config B: soft={config_b.soft_timeout}s, hard={config_b.hard_timeout}s (Aggressive)\"\n",
    ")\n",
    "\n",
    "comparison = data_manager.compare_configurations(config_a, config_b, n_sims=10000)\n",
    "\n",
    "# Distribution comparison visualization\n",
    "fig = StatisticalComparisonPanel.create_distribution_comparison(\n",
    "    comparison[\"df_a\"], comparison[\"df_b\"], (\"Default (15s/30s)\", \"Aggressive (8s/15s)\")\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# Effect size visualization\n",
    "fig2 = StatisticalComparisonPanel.create_effect_size_chart(comparison)\n",
    "fig2.show()\n",
    "\n",
    "# Print statistical summary\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üìä STATISTICAL COMPARISON RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "lat = comparison[\"latency\"]\n",
    "qual = comparison[\"quality\"]\n",
    "print(f\"\\n‚è±Ô∏è LATENCY: p={lat['p_value']:.2e}, Cohen's d={lat['cohens_d']:.3f}\")\n",
    "print(f\"‚≠ê QUALITY: p={qual['p_value']:.2e}, Cohen's d={qual['cohens_d']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéì Summary & Launch Full Dashboard\n",
    "\n",
    "For the complete interactive experience with real-time controls, sliders, and live updates:\n",
    "\n",
    "```bash\n",
    "# From the project root directory\n",
    "python run_dashboard.py\n",
    "```\n",
    "\n",
    "Then open **http://localhost:8050** in your browser.\n",
    "\n",
    "### Dashboard Features:\n",
    "- üìä **System Monitor** - Real-time agent health and throughput\n",
    "- üî¨ **Sensitivity Analysis** - Interactive parameter exploration\n",
    "- üéØ **Pareto Explorer** - 3D quality-latency surfaces\n",
    "- üìê **A/B Testing** - Statistical comparison tools\n",
    "- üé≤ **Monte Carlo** - Simulation controls\n",
    "\n",
    "---\n",
    "*MIT-Level Research Dashboard | Multi-Agent Tour Guide System | November 2025*\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
