"""
üìê Information-Theoretic Analysis for Multi-Agent Systems
==========================================================

MIT-Level Innovation: Fundamental Bounds on System Performance

This module develops information-theoretic foundations for understanding
the fundamental limits of multi-agent content selection systems. We derive:
- Lower bounds on achievable regret
- Channel capacity for user satisfaction communication
- Mutual information between agents and outcomes
- Rate-distortion analysis for content quality

Key Innovations:
1. Information-Theoretic Regret Bounds
2. Channel Capacity Analysis for Agent-User Communication
3. Mutual Information Framework for Agent Performance
4. Rate-Distortion Theory for Quality-Latency Tradeoffs
5. Entropy-Based Diversity Metrics

Academic References:
- Cover & Thomas (2006) "Elements of Information Theory"
- Russo & Van Roy (2016) "An Information-Theoretic Analysis of Thompson Sampling"
- Lattimore & Szepesv√°ri (2020) "Bandit Algorithms" (Chapter on Information-Theoretic Bounds)
- Berger (1971) "Rate Distortion Theory"

Author: MIT-Level Research Framework
Version: 1.0.0
Date: November 2025
"""

from __future__ import annotations

import math
from dataclasses import dataclass
from datetime import datetime
from typing import Any

import numpy as np
from scipy.integrate import quad

# =============================================================================
# Core Information-Theoretic Quantities
# =============================================================================


class EntropyCalculator:
    """
    Calculate various entropy measures for discrete and continuous distributions.

    Shannon Entropy: H(X) = -Œ£ p(x) log p(x)
    - Measures uncertainty/information content
    - Maximum for uniform distribution
    - Zero for deterministic distribution
    """

    @staticmethod
    def shannon_entropy(probs: np.ndarray, base: float = 2.0) -> float:
        """
        Compute Shannon entropy H(X).

        H(X) = -Œ£·µ¢ p(x·µ¢) log p(x·µ¢)

        Args:
            probs: Probability distribution (must sum to 1)
            base: Logarithm base (2 for bits, e for nats)

        Returns:
            Shannon entropy in specified units
        """
        probs = np.array(probs)
        probs = probs[probs > 0]  # Avoid log(0)

        if base == 2:
            return -np.sum(probs * np.log2(probs))
        elif base == np.e:
            return -np.sum(probs * np.log(probs))
        else:
            return -np.sum(probs * np.log(probs)) / np.log(base)

    @staticmethod
    def differential_entropy(
        pdf: callable, support: tuple[float, float] = (-np.inf, np.inf)
    ) -> float:
        """
        Compute differential entropy for continuous distribution.

        h(X) = -‚à´ f(x) log f(x) dx

        Args:
            pdf: Probability density function
            support: Integration bounds

        Returns:
            Differential entropy in nats
        """

        def integrand(x):
            p = pdf(x)
            if p <= 0:
                return 0
            return -p * np.log(p)

        result, _ = quad(integrand, support[0], support[1])
        return result

    @staticmethod
    def gaussian_entropy(variance: float) -> float:
        """
        Entropy of Gaussian distribution N(Œº, œÉ¬≤).

        h(X) = (1/2) log(2œÄeœÉ¬≤)

        Note: Independent of mean Œº.
        """
        return 0.5 * np.log(2 * np.pi * np.e * variance)

    @staticmethod
    def conditional_entropy(joint_probs: np.ndarray) -> float:
        """
        Compute conditional entropy H(Y|X).

        H(Y|X) = -Œ£·µ¢‚±º p(x·µ¢, y‚±º) log p(y‚±º|x·µ¢)
               = H(X,Y) - H(X)

        Args:
            joint_probs: Joint probability matrix P(X,Y)

        Returns:
            Conditional entropy H(Y|X)
        """
        # Joint entropy H(X,Y)
        joint_probs = np.array(joint_probs)
        h_joint = EntropyCalculator.shannon_entropy(joint_probs.flatten())

        # Marginal entropy H(X)
        p_x = joint_probs.sum(axis=1)
        h_x = EntropyCalculator.shannon_entropy(p_x)

        return h_joint - h_x


class MutualInformationCalculator:
    """
    Calculate mutual information between random variables.

    I(X;Y) = H(X) + H(Y) - H(X,Y)
           = H(X) - H(X|Y)
           = H(Y) - H(Y|X)
           = Œ£·µ¢‚±º p(x·µ¢,y‚±º) log[p(x·µ¢,y‚±º)/(p(x·µ¢)p(y‚±º))]

    Mutual information measures:
    - Reduction in uncertainty of X given Y
    - Information shared between X and Y
    - Always non-negative (I ‚â• 0)
    - Symmetric (I(X;Y) = I(Y;X))
    """

    @staticmethod
    def compute(joint_probs: np.ndarray) -> float:
        """
        Compute mutual information I(X;Y) from joint distribution.

        Args:
            joint_probs: Joint probability matrix P(X,Y)

        Returns:
            Mutual information in bits
        """
        joint = np.array(joint_probs)

        # Marginals
        p_x = joint.sum(axis=1, keepdims=True)
        p_y = joint.sum(axis=0, keepdims=True)

        # Avoid division by zero
        outer = p_x @ p_y
        mask = (joint > 0) & (outer > 0)

        mi = np.sum(joint[mask] * np.log2(joint[mask] / outer[mask]))

        return max(0, mi)  # MI is always non-negative

    @staticmethod
    def normalized(joint_probs: np.ndarray) -> float:
        """
        Compute normalized mutual information (NMI).

        NMI = I(X;Y) / min(H(X), H(Y))

        NMI ‚àà [0, 1], equals 1 when X and Y are perfectly correlated.
        """
        mi = MutualInformationCalculator.compute(joint_probs)

        p_x = np.array(joint_probs).sum(axis=1)
        p_y = np.array(joint_probs).sum(axis=0)

        h_x = EntropyCalculator.shannon_entropy(p_x)
        h_y = EntropyCalculator.shannon_entropy(p_y)

        normalizer = min(h_x, h_y)
        if normalizer == 0:
            return 0.0

        return mi / normalizer

    @staticmethod
    def conditional(
        joint_xyz: np.ndarray,  # P(X, Y, Z)
    ) -> float:
        """
        Compute conditional mutual information I(X;Y|Z).

        I(X;Y|Z) = H(X|Z) + H(Y|Z) - H(X,Y|Z)
                 = I(X;Y,Z) - I(X;Z)

        Measures information X shares with Y that is not shared with Z.
        """
        # Sum over Z to get marginals
        p_xz = joint_xyz.sum(axis=1)  # P(X,Z)
        p_yz = joint_xyz.sum(axis=0)  # P(Y,Z)
        joint_xyz.sum(axis=(0, 1))  # P(Z)

        # This is a simplified computation
        # Full implementation would require careful handling
        h_x_given_z = EntropyCalculator.conditional_entropy(p_xz)
        h_y_given_z = EntropyCalculator.conditional_entropy(p_yz)

        # Approximate via chain rule
        return max(0, h_x_given_z + h_y_given_z - h_x_given_z)


class KLDivergence:
    """
    Kullback-Leibler Divergence (Relative Entropy).

    D_KL(P || Q) = Œ£·µ¢ p(x·µ¢) log[p(x·µ¢)/q(x·µ¢)]

    Properties:
    - Non-negative: D_KL ‚â• 0 (Gibbs' inequality)
    - Zero iff P = Q
    - Not symmetric: D_KL(P||Q) ‚â† D_KL(Q||P) generally
    - Measures "information lost" when Q is used to approximate P
    """

    @staticmethod
    def compute(p: np.ndarray, q: np.ndarray) -> float:
        """
        Compute KL divergence D_KL(P || Q).

        Args:
            p: True distribution
            q: Approximating distribution

        Returns:
            KL divergence in nats
        """
        p = np.array(p)
        q = np.array(q)

        # Handle zeros
        mask = p > 0

        if not np.all(q[mask] > 0):
            return float("inf")  # KL is infinite if q=0 where p>0

        return np.sum(p[mask] * np.log(p[mask] / q[mask]))

    @staticmethod
    def symmetric(p: np.ndarray, q: np.ndarray) -> float:
        """
        Compute symmetric KL divergence (Jensen-Shannon divergence).

        JSD(P || Q) = [D_KL(P || M) + D_KL(Q || M)] / 2
        Where M = (P + Q) / 2
        """
        p = np.array(p)
        q = np.array(q)
        m = (p + q) / 2

        return (KLDivergence.compute(p, m) + KLDivergence.compute(q, m)) / 2


# =============================================================================
# Information-Theoretic Regret Bounds
# =============================================================================


@dataclass
class RegretBoundResult:
    """Result of regret bound analysis."""

    bound_type: str
    bound_value: float
    parameters: dict[str, float]
    interpretation: str
    tightness: str  # "tight", "loose", "asymptotically_tight"


class InformationTheoreticRegretBounds:
    """
    Derive fundamental lower bounds on regret using information theory.

    Key Results:

    1. Lai-Robbins Lower Bound:
       lim inf_{T‚Üí‚àû} E[R(T)] / log(T) ‚â• Œ£·µ¢ Œî·µ¢ / D_KL(ŒΩ·µ¢ || ŒΩ*)

       Where:
       - Œî·µ¢ = Œº* - Œº·µ¢ (suboptimality gap)
       - D_KL(ŒΩ·µ¢ || ŒΩ*) = KL divergence between arm distributions

    2. Information-Directed Sampling Bound:
       E[R(T)] ‚â§ ‚àö(Œì_T ¬∑ I_T / 2)

       Where:
       - Œì_T = expected regret
       - I_T = information gain about optimal arm

    3. Russo-Van Roy Information-Theoretic Analysis:
       For Thompson Sampling with K arms, D-dimensional context:
       E[R(T)] ‚â§ O(D‚àö(KT log T))
    """

    def __init__(self, n_arms: int, arm_means: list[float] | None = None):
        """
        Initialize regret bound calculator.

        Args:
            n_arms: Number of arms (agents)
            arm_means: True mean rewards (if known)
        """
        self.K = n_arms
        self.arm_means = arm_means or [0.5] * n_arms
        self.optimal_mean = max(self.arm_means)
        self.optimal_arm = self.arm_means.index(self.optimal_mean)

    def lai_robbins_bound(
        self, T: int, arm_variances: list[float] | None = None
    ) -> RegretBoundResult:
        """
        Compute Lai-Robbins lower bound on regret.

        For Gaussian arms with known variance œÉ¬≤:
        D_KL(ŒΩ_a || ŒΩ*) = (Œº* - Œº_a)¬≤ / (2œÉ¬≤)

        Lower bound:
        lim inf E[R(T)] / log(T) ‚â• Œ£_{a: Œº_a < Œº*} Œî_a / D_KL(ŒΩ_a || ŒΩ*)
                                 = Œ£_{a: Œº_a < Œº*} 2œÉ¬≤ / Œî_a

        Args:
            T: Time horizon
            arm_variances: Variance for each arm

        Returns:
            Regret bound result
        """
        variances = arm_variances or [0.25] * self.K  # Default: œÉ¬≤ = 0.25

        bound_sum = 0.0
        for a in range(self.K):
            if a == self.optimal_arm:
                continue

            delta_a = self.optimal_mean - self.arm_means[a]
            if delta_a > 0:
                # KL divergence for Gaussian
                kl = delta_a**2 / (2 * variances[a])
                if kl > 0:
                    bound_sum += delta_a / kl

        # Asymptotic bound: grows as log(T)
        bound = bound_sum * math.log(T)

        return RegretBoundResult(
            bound_type="Lai-Robbins Lower Bound",
            bound_value=bound,
            parameters={
                "T": T,
                "K": self.K,
                "optimal_mean": self.optimal_mean,
                "min_gap": min(
                    self.optimal_mean - m
                    for m in self.arm_means
                    if m < self.optimal_mean
                )
                if any(m < self.optimal_mean for m in self.arm_means)
                else 0,
            },
            interpretation=(
                f"Any consistent policy must incur at least {bound:.2f} regret "
                f"over {T} rounds. This is a fundamental limit."
            ),
            tightness="asymptotically_tight",
        )

    def thompson_sampling_upper_bound(
        self, T: int, delta: float = 0.05
    ) -> RegretBoundResult:
        """
        Upper bound for Thompson Sampling (Russo-Van Roy analysis).

        For K-armed bandits:
        E[R(T)] ‚â§ O(‚àö(KT log K))

        More precisely:
        E[R(T)] ‚â§ C ¬∑ ‚àö(KT ¬∑ log(K))

        Where C depends on the problem structure.
        """
        # Theoretical constant (simplified)
        C = 2.0

        bound = C * math.sqrt(self.K * T * math.log(max(2, self.K)))

        return RegretBoundResult(
            bound_type="Thompson Sampling Upper Bound",
            bound_value=bound,
            parameters={
                "T": T,
                "K": self.K,
                "constant": C,
            },
            interpretation=(
                f"Thompson Sampling achieves regret at most {bound:.2f} "
                f"over {T} rounds with high probability."
            ),
            tightness="tight",
        )

    def information_ratio_bound(
        self, T: int, entropy_bound: float = 2.0
    ) -> RegretBoundResult:
        """
        Information ratio bound (Russo-Van Roy).

        Define information ratio:
        Œì_t = (E[Œî_{A_t}])¬≤ / I(A*; (A_t, R_t) | H_{t-1})

        If Œì_t ‚â§ Œì for all t, then:
        E[R(T)] ‚â§ ‚àö(2 Œì T H(A*))

        Where H(A*) ‚â§ log(K) is the entropy of the optimal arm.
        """
        # Information ratio is problem-dependent
        # For many problems, Œì ‚â§ 1/2
        Gamma = 0.5

        # Entropy of optimal arm (worst case: log K)
        H_Astar = math.log(self.K)

        # Bound
        bound = math.sqrt(2 * Gamma * T * H_Astar)

        return RegretBoundResult(
            bound_type="Information Ratio Bound",
            bound_value=bound,
            parameters={
                "T": T,
                "K": self.K,
                "info_ratio": Gamma,
                "entropy_bound": H_Astar,
            },
            interpretation=(
                f"With information ratio Œì={Gamma:.2f}, expected regret "
                f"is bounded by {bound:.2f}."
            ),
            tightness="tight",
        )

    def minimax_bound(self, T: int) -> RegretBoundResult:
        """
        Minimax regret bound.

        inf_œÄ sup_ŒΩ E_ŒΩ[R_œÄ(T)] ‚â• c ¬∑ ‚àö(KT)

        No algorithm can do better than O(‚àö(KT)) in the worst case.
        """
        # Minimax constant
        c = 1.0 / (2 * math.sqrt(2))

        bound = c * math.sqrt(self.K * T)

        return RegretBoundResult(
            bound_type="Minimax Lower Bound",
            bound_value=bound,
            parameters={
                "T": T,
                "K": self.K,
                "constant": c,
            },
            interpretation=(
                f"No algorithm can achieve worst-case regret better than "
                f"{bound:.2f} over {T} rounds."
            ),
            tightness="tight",
        )


# =============================================================================
# Channel Capacity for Agent-User Communication
# =============================================================================


class AgentUserChannel:
    """
    Model the agent-user interaction as an information channel.

    Channel: Agent Selection ‚Üí Content Presentation ‚Üí User Satisfaction

    Channel capacity C = max_{P(X)} I(X; Y)

    This gives the maximum achievable rate of "satisfaction bits" per selection.
    """

    def __init__(self, n_agents: int = 3, satisfaction_levels: int = 5):
        """
        Initialize channel model.

        Args:
            n_agents: Number of input symbols (agents)
            satisfaction_levels: Number of output symbols (satisfaction levels)
        """
        self.n_input = n_agents
        self.n_output = satisfaction_levels

    def compute_capacity(
        self, transition_matrix: np.ndarray
    ) -> tuple[float, np.ndarray]:
        """
        Compute channel capacity using Blahut-Arimoto algorithm.

        Channel capacity:
        C = max_{P(x)} I(X; Y)

        The Blahut-Arimoto algorithm iteratively solves:
        q(x) ‚àù exp(Œ£_y p(y|x) log[p(y|x) / Œ£_{x'} q(x') p(y|x')])

        Args:
            transition_matrix: P(Y|X) matrix, shape (n_input, n_output)

        Returns:
            (capacity in bits, optimal input distribution)
        """
        P = np.array(transition_matrix)
        n_x, n_y = P.shape

        # Initialize uniform input distribution
        q = np.ones(n_x) / n_x

        # Blahut-Arimoto iterations
        max_iter = 1000
        tol = 1e-10

        for _ in range(max_iter):
            # Compute output distribution
            r = P.T @ q  # r[y] = Œ£_x q[x] P[y|x]
            r = np.maximum(r, 1e-15)

            # Compute new input distribution
            # q_new[x] ‚àù exp(Œ£_y P[y|x] log(P[y|x] / r[y]))
            log_ratio = np.zeros((n_x, n_y))
            for x in range(n_x):
                for y in range(n_y):
                    if P[x, y] > 0:
                        log_ratio[x, y] = P[x, y] * np.log(P[x, y] / r[y])

            exponent = log_ratio.sum(axis=1)
            q_new = np.exp(exponent)
            q_new /= q_new.sum()

            # Check convergence
            if np.max(np.abs(q_new - q)) < tol:
                break

            q = q_new

        # Compute mutual information at optimal q
        capacity = MutualInformationCalculator.compute(np.outer(q, np.ones(n_y)) * P)

        return capacity, q

    def noisy_channel_capacity(self, noise_level: float = 0.1) -> float:
        """
        Compute capacity of a noisy agent-satisfaction channel.

        Models situation where user satisfaction has some noise/randomness
        independent of the agent selection.

        For binary symmetric channel with error probability Œµ:
        C = 1 - H(Œµ)

        Args:
            noise_level: Probability of "flipped" satisfaction

        Returns:
            Channel capacity
        """
        if noise_level == 0:
            return np.log2(self.n_input)
        elif noise_level >= 0.5:
            return 0.0

        # Binary entropy function
        h_epsilon = -(
            noise_level * np.log2(noise_level)
            + (1 - noise_level) * np.log2(1 - noise_level)
        )

        return np.log2(self.n_input) - h_epsilon


# =============================================================================
# Rate-Distortion Analysis for Quality-Latency Tradeoff
# =============================================================================


class RateDistortionAnalyzer:
    """
    Rate-Distortion Theory for understanding Quality-Latency tradeoffs.

    Rate-Distortion function R(D):
    R(D) = min_{P(Y|X): E[d(X,Y)]‚â§D} I(X; Y)

    This gives the minimum "description rate" (latency/processing) needed
    to achieve a given "distortion" (quality loss).

    For multi-agent systems:
    - X = ideal content selection
    - Y = actual content delivered (under time constraints)
    - d(X, Y) = quality loss from suboptimal selection
    - R = processing/waiting time
    """

    def __init__(self, alphabet_size: int = 3):
        """
        Initialize rate-distortion analyzer.

        Args:
            alphabet_size: Size of source alphabet (number of agents)
        """
        self.K = alphabet_size

    def gaussian_rd_function(self, source_variance: float, distortion: float) -> float:
        """
        Rate-distortion function for Gaussian source with squared error.

        For X ~ N(0, œÉ¬≤) with d(x,y) = (x-y)¬≤:
        R(D) = max(0, (1/2) log(œÉ¬≤/D))

        This is the fundamental limit: cannot do better than this rate
        for the given distortion.

        Args:
            source_variance: Variance of the source
            distortion: Target distortion level

        Returns:
            Minimum rate in nats
        """
        if distortion >= source_variance:
            return 0.0

        return 0.5 * np.log(source_variance / distortion)

    def discrete_rd_function(
        self,
        source_probs: np.ndarray,
        distortion_matrix: np.ndarray,
        target_distortion: float,
    ) -> float:
        """
        Compute rate-distortion function for discrete source.

        Uses Blahut algorithm to minimize I(X;Y) subject to E[d(X,Y)] ‚â§ D.

        Args:
            source_probs: P(X) source distribution
            distortion_matrix: d[x,y] distortion for each (x,y) pair
            target_distortion: Maximum allowed average distortion

        Returns:
            Minimum rate in bits
        """
        p = np.array(source_probs)
        d = np.array(distortion_matrix)
        n_x, n_y = d.shape

        # Lagrangian: minimize I(X;Y) + Œª(E[d] - D)
        # Use bisection on Œª to find correct distortion

        def compute_rd_at_lambda(lam):
            """Compute rate and distortion at given Lagrange multiplier."""
            # Optimal conditional distribution
            # q(y|x) ‚àù exp(-Œª d(x,y))
            log_q = -lam * d
            q = np.exp(log_q - log_q.max(axis=1, keepdims=True))
            q /= q.sum(axis=1, keepdims=True)

            # Marginal on Y
            p @ q

            # Rate = I(X;Y)
            joint = np.outer(p, np.ones(n_y)) * q
            rate = MutualInformationCalculator.compute(joint)

            # Distortion
            dist = np.sum(p[:, None] * q * d)

            return rate, dist

        # Binary search for Œª that gives target distortion
        lambda_low, lambda_high = 0.001, 100.0

        for _ in range(50):
            lambda_mid = (lambda_low + lambda_high) / 2
            _, dist = compute_rd_at_lambda(lambda_mid)

            if dist > target_distortion:
                lambda_low = lambda_mid
            else:
                lambda_high = lambda_mid

        rate, _ = compute_rd_at_lambda(lambda_mid)
        return rate

    def compute_pareto_frontier(
        self, n_points: int = 50
    ) -> tuple[list[float], list[float]]:
        """
        Compute the rate-distortion Pareto frontier.

        Returns points on the curve showing achievable (rate, distortion) pairs.

        Returns:
            (rates, distortions) lists
        """
        # For uniform source, Hamming distortion
        p = np.ones(self.K) / self.K
        d = 1 - np.eye(self.K)  # Hamming distance

        # Maximum distortion (achievable at rate 0)
        d_max = (self.K - 1) / self.K

        distortions = np.linspace(0.001, d_max * 0.99, n_points)
        rates = []

        for D in distortions:
            R = self.discrete_rd_function(p, d, D)
            rates.append(R)

        return rates, list(distortions)


# =============================================================================
# Diversity and Coverage Metrics
# =============================================================================


class DiversityMetrics:
    """
    Information-theoretic metrics for content diversity.

    A good multi-agent system should provide diverse content across:
    - Different content types (video, music, text)
    - Different topics and themes
    - Different user preferences

    Entropy-based metrics capture this diversity naturally.
    """

    @staticmethod
    def selection_entropy(selection_counts: dict[str, int]) -> float:
        """
        Compute entropy of agent selections.

        Higher entropy = more diverse selections.
        Maximum entropy = log(K) when selections are uniform.

        Args:
            selection_counts: Dict mapping agent -> number of selections

        Returns:
            Selection entropy in bits
        """
        total = sum(selection_counts.values())
        if total == 0:
            return 0.0

        probs = np.array(list(selection_counts.values())) / total
        return EntropyCalculator.shannon_entropy(probs)

    @staticmethod
    def normalized_diversity(selection_counts: dict[str, int]) -> float:
        """
        Compute normalized diversity score.

        Normalized to [0, 1] where:
        - 0 = all selections same agent
        - 1 = uniform distribution over agents

        Formula: H(X) / log(K)
        """
        K = len(selection_counts)
        if K <= 1:
            return 0.0

        entropy = DiversityMetrics.selection_entropy(selection_counts)
        max_entropy = np.log2(K)

        return entropy / max_entropy

    @staticmethod
    def conditional_diversity(
        selections_by_context: dict[str, dict[str, int]],
    ) -> float:
        """
        Compute conditional diversity: diversity given context.

        Measures: Are selections diverse within each context type?

        Args:
            selections_by_context: context -> (agent -> count)

        Returns:
            Average conditional entropy
        """
        total_count = 0
        weighted_entropy = 0.0

        for _context, counts in selections_by_context.items():
            context_total = sum(counts.values())
            total_count += context_total

            entropy = DiversityMetrics.selection_entropy(counts)
            weighted_entropy += context_total * entropy

        if total_count == 0:
            return 0.0

        return weighted_entropy / total_count

    @staticmethod
    def information_gain(
        prior_counts: dict[str, int], posterior_counts: dict[str, int]
    ) -> float:
        """
        Compute information gain from updating beliefs.

        IG = H(prior) - H(posterior)

        Positive IG means we gained information (reduced uncertainty).
        """
        h_prior = DiversityMetrics.selection_entropy(prior_counts)
        h_posterior = DiversityMetrics.selection_entropy(posterior_counts)

        return h_prior - h_posterior


# =============================================================================
# Analysis Report Generator
# =============================================================================


@dataclass
class InformationTheoreticAnalysis:
    """Complete information-theoretic analysis report."""

    timestamp: datetime
    n_agents: int
    n_observations: int

    # Entropy measures
    selection_entropy: float
    normalized_diversity: float

    # Regret bounds
    lai_robbins_bound: RegretBoundResult
    thompson_bound: RegretBoundResult
    minimax_bound: RegretBoundResult

    # Channel analysis
    channel_capacity: float

    # Rate-distortion
    rd_frontier: tuple[list[float], list[float]]

    def to_dict(self) -> dict[str, Any]:
        """Convert to dictionary."""
        return {
            "timestamp": self.timestamp.isoformat(),
            "n_agents": self.n_agents,
            "n_observations": self.n_observations,
            "selection_entropy": self.selection_entropy,
            "normalized_diversity": self.normalized_diversity,
            "channel_capacity": self.channel_capacity,
            "regret_bounds": {
                "lai_robbins": self.lai_robbins_bound.bound_value,
                "thompson": self.thompson_bound.bound_value,
                "minimax": self.minimax_bound.bound_value,
            },
        }


class InformationTheoreticAnalyzer:
    """
    Complete information-theoretic analysis for multi-agent systems.
    """

    def __init__(self, n_agents: int = 3, arm_means: list[float] | None = None):
        """Initialize analyzer."""
        self.n_agents = n_agents
        self.arm_means = arm_means or [0.7, 0.5, 0.6]  # video, music, text

        self.regret_bounds = InformationTheoreticRegretBounds(n_agents, arm_means)
        self.channel = AgentUserChannel(n_agents)
        self.rd_analyzer = RateDistortionAnalyzer(n_agents)

    def analyze(
        self, selection_counts: dict[str, int], T: int = 1000
    ) -> InformationTheoreticAnalysis:
        """
        Run complete information-theoretic analysis.

        Args:
            selection_counts: Agent selection counts
            T: Time horizon for regret bounds

        Returns:
            Complete analysis report
        """
        # Diversity metrics
        sel_entropy = DiversityMetrics.selection_entropy(selection_counts)
        norm_div = DiversityMetrics.normalized_diversity(selection_counts)

        # Regret bounds
        lr_bound = self.regret_bounds.lai_robbins_bound(T)
        ts_bound = self.regret_bounds.thompson_sampling_upper_bound(T)
        mm_bound = self.regret_bounds.minimax_bound(T)

        # Channel capacity (with typical transition matrix)
        P = np.array(
            [
                [0.6, 0.2, 0.1, 0.05, 0.05],  # video
                [0.4, 0.3, 0.15, 0.1, 0.05],  # music
                [0.5, 0.25, 0.15, 0.05, 0.05],  # text
            ]
        )
        capacity, _ = self.channel.compute_capacity(P)

        # Rate-distortion frontier
        rd_frontier = self.rd_analyzer.compute_pareto_frontier()

        return InformationTheoreticAnalysis(
            timestamp=datetime.now(),
            n_agents=self.n_agents,
            n_observations=sum(selection_counts.values()),
            selection_entropy=sel_entropy,
            normalized_diversity=norm_div,
            lai_robbins_bound=lr_bound,
            thompson_bound=ts_bound,
            minimax_bound=mm_bound,
            channel_capacity=capacity,
            rd_frontier=rd_frontier,
        )


# =============================================================================
# Example Usage
# =============================================================================


def demo_information_theory():
    """Demonstrate information-theoretic analysis."""
    print("=" * 70)
    print("üìê INFORMATION-THEORETIC ANALYSIS DEMO")
    print("=" * 70)

    # Entropy calculations
    print("\nüìä Entropy Calculations:")
    probs = [0.6, 0.25, 0.15]  # Selection probabilities
    entropy = EntropyCalculator.shannon_entropy(probs)
    print(f"   Selection entropy: {entropy:.4f} bits")
    print(f"   Maximum entropy: {np.log2(3):.4f} bits")
    print(f"   Normalized diversity: {entropy / np.log2(3):.4f}")

    # Mutual information
    print("\nüìä Mutual Information:")
    joint = np.array(
        [
            [0.4, 0.1, 0.05],
            [0.1, 0.2, 0.05],
            [0.05, 0.05, 0.0],
        ]
    )
    mi = MutualInformationCalculator.compute(joint)
    print(f"   I(Agent; Satisfaction): {mi:.4f} bits")

    # Regret bounds
    print("\nüìä Regret Bounds (T=1000):")
    analyzer = InformationTheoreticAnalyzer(3, [0.7, 0.5, 0.6])

    lr = analyzer.regret_bounds.lai_robbins_bound(1000)
    print(f"   Lai-Robbins Lower Bound: {lr.bound_value:.2f}")

    ts = analyzer.regret_bounds.thompson_sampling_upper_bound(1000)
    print(f"   Thompson Sampling Upper: {ts.bound_value:.2f}")

    mm = analyzer.regret_bounds.minimax_bound(1000)
    print(f"   Minimax Lower Bound: {mm.bound_value:.2f}")

    # Channel capacity
    print("\nüìä Channel Capacity:")
    P = np.array(
        [
            [0.6, 0.2, 0.1, 0.05, 0.05],
            [0.4, 0.3, 0.15, 0.1, 0.05],
            [0.5, 0.25, 0.15, 0.05, 0.05],
        ]
    )
    capacity, opt_input = analyzer.channel.compute_capacity(P)
    print(f"   Channel capacity: {capacity:.4f} bits")
    print(f"   Optimal input dist: {opt_input}")

    print("\n‚úÖ Demo complete!")


if __name__ == "__main__":
    demo_information_theory()
